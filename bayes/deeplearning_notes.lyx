#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "lmodern" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\rightmargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Part
Deep learning 
\end_layout

\begin_layout Section
Linear Regression with an ML approach - a gentle introduction to ML ideas
\end_layout

\begin_layout Standard
If we were to apply deep learning to the regression problem, then for input:
 
\begin_inset Formula $x\in\mathbb{R}^{n}$
\end_inset

 , we need to predict 
\begin_inset Formula $y$
\end_inset

 for 
\begin_inset Formula $x$
\end_inset

 as 
\begin_inset Formula $\hat{y}$
\end_inset

, so we can write 
\begin_inset Formula $\hat{y}=\mathbf{w}^{T}\mathbf{x}$
\end_inset

.
 Here,
\begin_inset Formula $\mathbf{w}\in\mathbb{R}^{n}$
\end_inset

 are parameters of this prediction model.
 Before we define a performance measure of our classification algorithm,
 it is useful to think of the design matrix of 
\begin_inset Formula $m$
\end_inset

 example points - that are not to be used for training but only for the
 purpose of performance measurement.
 Using these example points, one may use MSE criteria and solve for 
\begin_inset Formula $w=(\boldsymbol{x}^{T}\boldsymbol{x})^{-1}\boldsymbol{y}$
\end_inset

 (using first-order conditions for 
\begin_inset Formula $MSE$
\end_inset

)
\begin_inset Foot
status open

\begin_layout Plain Layout
Bias and Variance are two measures for a point estimator.
 It's not trivial to choose between two estimators with disparate values
 of bias and variance (one with a high bias, low variance while the other
 with a low bias, high variance).
 MSE(
\begin_inset Formula $=Bias^{2}+Var$
\end_inset

) gets around this dilemma.
 Another way is cross-validation.
 Generalisation error measured with MSE results in increased variance and
 lowered bias as capacity is increased.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
However, what separates machine learning from optimization is that the former
 focusses on reducing generalization error i.e.
 reduction of error for unseen data.
 The 
\series bold
generalisation error
\series default
 is the expected value of the error on a new input i.e.
 
\begin_inset Formula $\frac{1}{m}\|\boldsymbol{x}_{test}\boldsymbol{w-}\boldsymbol{y}_{test}\|^{2}$
\end_inset

.
 The training and test data are generated with a dgp.
 An algorithm's capacity 
\end_layout

\begin_layout Standard
Moving further with the linear regression model, one can attempt to train
 the model (or solve for 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

) with the fitted equation being 
\begin_inset Formula $\hat{y}=b+w_{1}x+w_{2}x^{2}$
\end_inset

.
 A nonparametric model takes a different approach where the parameter vector
 is not necessarily (even) finite.
 One such example could be the nearest neighbour regression which returns
 
\begin_inset Formula $\hat{y}$
\end_inset

 as 
\begin_inset Formula $\hat{y}=y_{i}|i=argmin\|X_{i,:}-\boldsymbol{x}\|^{2}$
\end_inset

.
 A nonparametric algorithm can also be created by wrapping a parametric
 model into another algorithm that augments the parameters of the model
 it wraps.
 
\end_layout

\begin_layout Standard
Note tha even an ideal model - which is one that knows the true probability
 distribution that generates the data - would have some error due to noise
 in the distribution itself.
 This is called the 
\series bold
Bayes error
\series default
.
\end_layout

\begin_layout Standard
Before moving further, there is an admission to make about training algorithm.
 Our claim that we can minimise the generalistation error by only having
 considered the training data seems to go against the logical argument that
 all points must be considered in order to infer a parameter set (which
 is a rule to be set for getting 
\begin_inset Formula $y$
\end_inset

 from 
\begin_inset Formula $x$
\end_inset

).
 This is the basis for Wolpert's so-called no-free lunch theorem - which
 states that the average over all possible dgp every classification algorithm
 has the same error rate when classifying previously unobserved points.
 The most sophisticated algorithm therefore has the same average performance
 (over all possible tasks) as merely predicting that every point belongs
 to the same class!
\begin_inset CommandInset citation
LatexCommand cite
key "deeplearning2016"

\end_inset


\end_layout

\begin_layout Standard
The free-lunch theorem also implies that we must design algorithims for
 a specific task.
 This can be achieved by assigning preferences to the learning algorithms.
 For our regression problem, we may add a weight decay and consider the
 criterion 
\begin_inset Formula $J(\boldsymbol{w})=MSE_{train}+\lambda\boldsymbol{w}^{T}\boldsymbol{w}$
\end_inset

 for gauging algorithm performance.
 Minimising 
\begin_inset Formula $J(\boldsymbol{w})$
\end_inset

 with a high 
\begin_inset Formula $\lambda$
\end_inset

for example forces the 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 to become smaller.
 This step is a so-called regularisation step - which affects only the generalis
ation error but not the training error.
 In the gi ven regerssion example, regularisation involves changing on 
\begin_inset Formula $\lambda$
\end_inset

 but not 
\begin_inset Formula $MSE_{train}$
\end_inset

.
\end_layout

\begin_layout Standard
The parameters such as 
\begin_inset Formula $\lambda$
\end_inset

 that control the algorithm are called 
\series bold
hyperparameters
\series default
.
 More particularly, the 
\begin_inset Formula $\lambda$
\end_inset

 is a capacity hyperparameter.
 Notice that we cannot use the test-data for the fitting a hyperparameter
 as the former is reserved for generalisation error minimisation.
 The subset of training set use for hyperparameters is called a 
\series bold
validation set
\series default
.
 The training set is and validation set therefore two disjoint subsets of
 the training data - 80-20 being the typical split between the training
 and validation sets.
\end_layout

\begin_layout Subsection
MLE
\end_layout

\begin_layout Standard
Because of its consistency properties
\begin_inset Foot
status open

\begin_layout Plain Layout
The property of consistency is contingent on i) the true distribution of
 the data (
\begin_inset Formula $p_{data}$
\end_inset

) being represented by the chosen model (
\begin_inset Formula $p_{model}$
\end_inset

) and ii) the set of parameters 
\begin_inset Formula $\theta$
\end_inset

 being unique for the true data distribution.
\end_layout

\end_inset

, the MLE estimator is a useful subsitute to the MSE based optimistaion
 approach.
 MLE estimator 
\begin_inset Formula $\theta_{ML}$
\end_inset

 for family of probability functions 
\begin_inset Formula $p_{model}$
\end_inset

 with parameters 
\begin_inset Formula $\theta$
\end_inset

 for input 
\begin_inset Formula $\boldsymbol{x}$
\end_inset

 (which is distributed with an unknown distribution 
\begin_inset Formula $p_{data}(\boldsymbol{x})$
\end_inset

) can be written as 
\begin_inset Formula $\theta_{ML}=argmax_{\theta}p_{model}(\mathbb{X};\theta)$
\end_inset

 i.e.
 the 
\begin_inset Formula $\theta$
\end_inset

that maximises the probability of the model over the observations/examples
\begin_inset Formula $\mathbb{X}=\{x^{(1)},x^{(2)},...\}$
\end_inset

.
 For numerical underflow and other reasons, the log formulation of this
 definition is more popular.
 With a further simplification, we can write 
\begin_inset Formula 
\begin{align}
\theta_{ML} & =argmax_{\theta}\mathbb{E}_{x\sim\hat{p}_{data}}log\ p_{model}(\boldsymbol{x};\theta)\label{eq:MLE}
\end{align}

\end_inset

Yet another formulation of the Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLE"

\end_inset

 is the minimisation of the KL divergence 
\begin_inset Formula $D_{KL}(\hat{p}_{data}\|p_{model})=argmax_{\theta}\mathbb{E}_{x\sim\hat{p}_{data}}(log\ p_{data}(x)-log\ p_{model}(\boldsymbol{x}))$
\end_inset

 which is equivalant to minimization of the quantity in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLE"

\end_inset

.
 
\end_layout

\begin_layout Standard
For our linear regression model (as well as supervised learning), a useful
 extension of Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:MLE"

\end_inset

 is a conditional expectation model so that 
\begin_inset Formula 
\begin{alignat*}{1}
\theta_{ML} & =argmax_{\theta}P(Y|X;\theta)=argmax_{\theta}\sum_{i=1}^{m}log\ P(\boldsymbol{y}^{(i)}|\boldsymbol{x}^{(i)};\theta)
\end{alignat*}

\end_inset

 which we can use to predict 
\begin_inset Formula $y$
\end_inset

 for a given 
\begin_inset Formula $x$
\end_inset

.
 Let's assume 
\begin_inset Formula $p_{model}$
\end_inset

 of the form 
\begin_inset Formula $p(\boldsymbol{y}|\boldsymbol{x})=N(y;\hat{y}(\boldsymbol{x};\boldsymbol{w}),\sigma^{2})$
\end_inset

 where 
\begin_inset Formula $\hat{y}(\boldsymbol{x};\boldsymbol{w})$
\end_inset

 is the prediction of the Gaussian mean and 
\begin_inset Formula $p(\boldsymbol{y}|\boldsymbol{x})$
\end_inset

 is assumed to have constance 
\begin_inset Formula $\sigma$
\end_inset

.
 With 
\begin_inset Formula $x$
\end_inset

 being i.i.d.
 the likelihood to be maximised is 
\begin_inset Formula 
\begin{alignat*}{1}
\sum_{i=1}^{m}log\ p(y^{(i)}|x^{(i)};\theta) & =-m\cdot log\sigma-\frac{m}{2}log(2\pi)-\sum_{i=1}^{m}\frac{\|\hat{y}^{(i)}-y^{(i)}\|^{2}}{2\sigma^{2}}
\end{alignat*}

\end_inset

This turns out to be the same as MSE estimator.
\end_layout

\begin_layout Standard
Other than being consistent, the MLE estimator is also effiicent i.e.
 it requires fewer example points to minimise generealistation error.
 The Cramer-Rao
\begin_inset space ~
\end_inset

lower bound shows the MLE has the lowest MSE of all estimators.
\end_layout

\begin_layout Subsection
The Bayesian estimator
\end_layout

\begin_layout Standard
The Bayes theorem being
\begin_inset Formula $P(Y_{k}|X_{k})=\frac{P(X_{k}/Y_{k})P(Y_{k})}{\sum_{i}P(X_{i}/Y_{i})P(Y_{i})}=\frac{P(X_{k}/Y_{k})P(Y_{k})}{P(X)}$
\end_inset

, a bayesian pdf can be defined in terms of a prior distribution 
\begin_inset Formula $p(\theta)$
\end_inset

 (i.e.
 a belief about the distribution of 
\begin_inset Formula $\theta$
\end_inset

) as 
\begin_inset Formula 
\begin{alignat*}{1}
p(\theta|x^{(1)},...,x^{(m)}) & =\frac{p(x^{(1)},...,x^{(m)}|\theta)p(\theta)}{p(x^{(1)},...,x^{(m)})}
\end{alignat*}

\end_inset

.
 Applying to the linear regression problem, we can think of the vector 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 so that 
\begin_inset Formula $\hat{y}=\boldsymbol{w}^{T}\boldsymbol{x}$
\end_inset

 or 
\begin_inset Formula $\hat{\boldsymbol{y}}^{(train)}=X^{(train)}\boldsymbol{w}$
\end_inset

.
 The Gaussian conditional distribution can then be written as 
\begin_inset Formula 
\begin{alignat*}{1}
p(\boldsymbol{y}^{(train)}|X^{(train)},\boldsymbol{w}) & =N(\boldsymbol{y}^{(train)};X^{(train)}\boldsymbol{w},\boldsymbol{I})\propto e^{-\frac{1}{2}(\boldsymbol{y}^{(train)}-X^{(train)}\boldsymbol{w})^{T}(\boldsymbol{y}^{(train)}-X^{(train)}\boldsymbol{w})}
\end{alignat*}

\end_inset

Since Gaussian distribution is a conjugate prior for a Gaussian distribution,
 we choose the prior 
\begin_inset Formula 
\begin{alignat*}{1}
p(w) & =N(w,\mu_{0},\Lambda_{0})\propto e^{-\frac{1}{2}(w-\mu_{0})^{T}\Lambda_{0}^{-1}(w-\mu_{0})}
\end{alignat*}

\end_inset

 (
\begin_inset Formula $\mu_{0},\Lambda_{0}$
\end_inset

 being the mean and covariance matrix for the prior distribution).
 The posterior distribution 
\begin_inset Formula $p(\boldsymbol{w}|\boldsymbol{X},\boldsymbol{y})\propto p(\boldsymbol{y}|\boldsymbol{X},\boldsymbol{w})p(\boldsymbol{w})$
\end_inset

 would also be a Gaussian pdf.
\end_layout

\begin_layout Standard
While it makes sense to estimate posterior 
\begin_inset Formula $\hat{y}$
\end_inset

 with a full-distribution on the prior 
\begin_inset Formula $\theta$
\end_inset

, a point estimate is more desirable for tractability.
 When that is the case, a criteria such as MAP (maximum a posteriori) can
 be used instead of using MLE.
 This is defined as 
\begin_inset Formula 
\begin{alignat*}{1}
\theta_{MAP} & =argmax_{\theta}p(\boldsymbol{\theta}|\boldsymbol{x})=argmax_{\theta}log\ p(\boldsymbol{x}|\boldsymbol{\theta})+log\ p(\boldsymbol{\theta})
\end{alignat*}

\end_inset


\end_layout

\begin_layout Subsection
Supervised Learning
\end_layout

\begin_layout Standard
A popular way of designing a supervised learning algorithm is based on estimatin
g
\begin_inset Formula $p(y|\boldsymbol{x})$
\end_inset

.
 A simple classification into 0 or 1, for example can be solved with a logistic
 regression - which esimates 
\begin_inset Formula $p(y=1|\boldsymbol{x};\boldsymbol{\theta})=\sigma(\boldsymbol{\theta}^{T}\boldsymbol{x})$
\end_inset

.
 The support vector machine approaches are similar to logistic regression
 in the sense that it is driven by a linear equation 
\begin_inset Formula $\boldsymbol{w}^{T}\boldsymbol{x}+b$
\end_inset

.
 However, there are no probabilities to be estimated in SVM - the positive
 and negative values of the function indicate the class identity.
 
\end_layout

\begin_layout Standard
The SVM linear function can be written in terms of dot products of the examples
 i.e.
 
\begin_inset Formula 
\begin{alignat}{1}
\boldsymbol{w}^{T}\boldsymbol{x}+b & =b+\sum_{i=1}^{m}\alpha_{i}\boldsymbol{x}^{T}x^{(i)}\label{eq:SVM}
\end{alignat}

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\boldsymbol{x}^{(i)}$
\end_inset

is a training example and 
\begin_inset Formula $\alpha$
\end_inset

is a vector of coefficients.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVM"

\end_inset

 can be simplified with a kernel function 
\begin_inset Formula $\boldsymbol{x}\rightarrow\phi(\boldsymbol{x})$
\end_inset

 so that the dot-product is replaced with 
\begin_inset Formula $k(\boldsymbol{x},\boldsymbol{x}^{(i)})=\phi(\boldsymbol{x})\cdot\phi(\boldsymbol{x})$
\end_inset

.
 The prediction function then becomes 
\begin_inset Formula $f(\boldsymbol{x})=b+\sum_{i}\alpha_{i}k(\boldsymbol{x},\boldsymbol{x}^{(i)})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "deeplearning"
options "plain"

\end_inset


\end_layout

\end_body
\end_document

